*************************End To End ML Project With Deployment by Krish ****************************


-> Tutorial 1: https://www.youtube.com/watch?v=Rv6UFGNmNZg
   
#Agenda: 
   1.Setup Github Repository
   ---Now in VS Code------
   2.New Environment 
     {command to create environment: <conda create -p venv python==3.9 -y>}
     {command to activate environment: <conda activate venv/>}
   3.Setup.py
     "With the help of setup.py we can build our entire Machine Learning Application as a package "
     After building setup.py file run command <pip install -r requirements.txt> to install the package.The 'e .'is                 indicating setup.py file and install the required package.
   4.Requirement.txt
   5. src folder (within this folder create __init__.py so that we can find src as a package, when setup.py(findpackages()) runs it can install packages.
      src(sorce is responsible for entire project development). Once Src folder runs and build it can install as a package like seaborn.


-> Tutorial 2: https://www.youtube.com/watch?v=sDWL30CzJT8&t=1s

#Agenda: 
  1. Inside the src folder we create two folders: components and pipeline with __init__.py files 
  2. Within component folder create files Data Ingestion(we create here train and test of Data), Data Transformation, Moder Trainner(train the Model). These      all files are used as training purposes as within Pipeline folder we create another file i,e. Trainning Pipeline that is use to trigger(call) all components as training purposes.
  3. Also create two files in src folder i.e, exception.py & logger.py


-> Tutorial 3: https://www.youtube.com/watch?v=gqqGdu1P2FM&list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG&index=4

#Agenda: (EDA And Model Training)
1. DATASET: Student Process Indicator.
2. Load Data Set, EDA and Model training file.
3. In Model_trainning we perform one hot encoding as there are very less no. of features. so we can convert into numerical features and later on we can apply Normalization, Standardization.
4. In utils folder we start modular coding based on these files (Model_trainning), In data ingestion file we'll done train test split.



-> Tutorial 4: https://www.youtube.com/watch?v=_0v1UK7smBc&t=22s

#Agenda: Data Ingestion Implementation Line By Line
1. In data Ingestion we read the code and split  the data into train & test.
2. In data_Ingestion file we create class dataingestionconfig, this class is used to store input variables of data.
3. Here also we use decorator named as @dataclass so that we can directly definew our class variables like train_data_path and test_data_path.
4. initiate_data_ingestion function is used when data is stored in databases and we have to read data from db.
 